{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5419a809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import pynvml\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b82c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#注意，DeBERTa开始，最大输入文本就没有限制了，很NB\n",
    "def temp_collate_fn(batch):\n",
    "    max_len = max([len(f[\"input_id\"]) for f in batch])\n",
    "    input_id = [f['input_id'] + [0] * (max_len - len(f['input_id'])) for f in batch]\n",
    "    input_mask = [[1] * len(f['input_id']) + [0] * (max_len - len(f['input_id'])) for f in batch]\n",
    "    tags = [f['tags'] for f in batch]\n",
    "    \n",
    "    input_id = torch.tensor(input_id)\n",
    "    input_mask = torch.tensor(input_mask)\n",
    "    tags = torch.tensor(tags)\n",
    "    \n",
    "    output = (input_id, input_mask, tags)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3c9877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791dd975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,feature_data,device,tags:str):    #考虑分词的\n",
    "    preds = []\n",
    "    dataloader = DataLoader(feature_data, batch_size=2, shuffle=False, collate_fn=temp_collate_fn, drop_last=True)\n",
    "    model.zero_grad()\n",
    "    for batch in dataloader:\n",
    "        model.eval()\n",
    "        inputs = {'input_id': batch[0].to(device),\n",
    "                  'input_mask': batch[1].to(device),\n",
    "              }\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs)     #产出一个list[list],大长度就是所有文档或句子，里面的长度是token_num\n",
    "            preds.extend(output[0])\n",
    "    \n",
    "    fenzi=0\n",
    "    fenmu=0\n",
    "    for num in range(len(preds)):        \n",
    "        y_true=feature_data[num]['tags']\n",
    "        y_pred=preds[num].argmax(dim=-1)\n",
    "        if y_true==y_pred:\n",
    "            fenzi += 1\n",
    "    fenmu=len(preds)\n",
    "    p=fenzi/fenmu\n",
    "    \n",
    "    print(tags+\"数据准确率为:\",p,\"数据总数:\",fenmu,\"判断对的数目:\",fenzi)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b79fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这里都按照enti_softmax的格式改了，没有crf\n",
    "def train(model, train_features, dev_features,train_batch_size:int, num_epoch, learning_rate,device):\n",
    "    def finetune(features, optimizer):\n",
    "        best_score = -1\n",
    "        train_dataloader = DataLoader(features, batch_size=train_batch_size, shuffle=True, collate_fn=temp_collate_fn, drop_last=True)\n",
    "        total_steps = int(len(train_dataloader) * num_epoch )\n",
    "        warmup_steps = int(total_steps * 0.06)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "        num_steps=0\n",
    "        for epoch in range(int(num_epoch)):\n",
    "            model.zero_grad()\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                model.train()\n",
    "                inputs = {'input_id': batch[0].to(device),\n",
    "                      'input_mask': batch[1].to(device),\n",
    "                      'tags':batch[2].to(device),\n",
    "                      }\n",
    "                outputs = model(**inputs)\n",
    "                loss = outputs[1] \n",
    "                loss.backward()\n",
    "                #print(step,loss)\n",
    "                #if loss>10 or torch.isnan(loss)==True:\n",
    "                #    print(step,loss)\n",
    "                if step%50==0:\n",
    "                    print(\"已读取的数据数：\",step*2)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                model.zero_grad()\n",
    "                num_steps += 1\n",
    "                if (step + 1) == len(train_dataloader) - 1 :\n",
    "                    print(\"第\",epoch,\"轮loss为:\",loss)\n",
    "                    dev_F1 = evaluate(model, dev_features,device, tags=\"dev\")\n",
    "                    if dev_F1 > best_score:\n",
    "                        best_score = dev_F1\n",
    "                        torch.save(model.state_dict(),'huifu_model.pth' )\n",
    "        return num_steps\n",
    "    \n",
    "    new_layer = [\"FNN\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in new_layer)], \"weight_decay\":0.01 },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in new_layer)], \"lr\": learning_rate},\n",
    "        ]\n",
    "    \n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=1.5e-5)               #这里的lr是预训练的学习率              \n",
    "    model.zero_grad()\n",
    "    finetune(train_features, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd3dbaa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9081632653061225"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "89/98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862cbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
